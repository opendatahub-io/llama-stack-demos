{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# Level 3: Agentic RAG\n",
    "\n",
    "This tutorial presents an example of executing queries with agentic RAG in Llama Stack. It shows how to initialize an agent with the RAG tool provided by Llama Stack and to invoke it such that retrieval from a vector DB is activated when necessary. The tutorial also covers document ingestion using the RAG tool.\n",
    "For a foundational (non-agentic) RAG tutorial, please refer to [Level1_foundational_RAG.ipynb](demos/rag_agentic/notebooks/Level1_foundational_RAG.ipynb).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Connecting to a llama-stack server.\n",
    "2. Indexing a collection of documents in a vector DB for later retrieval.\n",
    "3. Initializing the agent capable of retrieving content from vector DB via tool use.\n",
    "4. Launching the agent and using it to answer user queries during the inference step.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have a running instance of the Llama Stack server (local or remote) with at least one preconfigured vector DB. For more information, please refer to the corresponding [Llama Stack tutorials](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "- Import the necessary libraries.\n",
    "- Define the settings for the RAG pipeline, including the Llama Stack server URL, inference and document ingestion parameters.\n",
    "- Initialize the connection to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "854e7cb4-aed9-4098-adc1-a66f4c9e6ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "from llama_stack_client import Agent, AgentEventLogger, RAGDocument, LlamaStackClient\n",
    "\n",
    "# the server endpoint\n",
    "LLAMA_STACK_SERVER_URL = \"http://localhost:8321\"\n",
    "\n",
    "# inference settings\n",
    "MODEL_ID = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant. \"\n",
    "TEMPERATURE = 0.0\n",
    "TOP_P = 0.95\n",
    "\n",
    "# RAG settings\n",
    "VECTOR_DB_EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "VECTOR_DB_EMBEDDING_DIMENSION = 384\n",
    "VECTOR_DB_CHUNK_SIZE = 512\n",
    "\n",
    "# For this demo, we are using Milvus Lite, which is our preferred solution. Any other Vector DB supported by Llama Stack can be used.\n",
    "VECTOR_DB_PROVIDER_ID = 'milvus'\n",
    "\n",
    "# initialize the inference strategy\n",
    "if TEMPERATURE > 0.0:\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": TEMPERATURE, \"top_p\": TOP_P}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "    \n",
    "# initialize the document collection to be used for RAG\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "    \n",
    "# initialize the server connection\n",
    "client = LlamaStackClient(base_url=os.environ.get(\"LLAMA_STACK_ENDPOINT\", LLAMA_STACK_SERVER_URL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203de51-f570-44ab-8130-36333a54888b",
   "metadata": {},
   "source": [
    "## 2. Indexing the Documents\n",
    "- Initialize a new document collection in the target vector DB. All parameters related to the vector DB, such as the embedding model and dimension, must be specified here.\n",
    "- Provide a list of document URLs to the RAG tool. Llama Stack will handle fetching, conversion and chunking of the documents' content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=VECTOR_DB_EMBEDDING_MODEL,\n",
    "    embedding_dimension=VECTOR_DB_EMBEDDING_DIMENSION,\n",
    "    provider_id=VECTOR_DB_PROVIDER_ID,\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    (\"https://www.openshift.guide/openshift-guide-screen.pdf\", \"application/pdf\"),\n",
    "    (\"https://www.cdflaborlaw.com/_images/content/2023_OCBJ_GC_Awards_Article.pdf\", \"application/pdf\"),\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=VECTOR_DB_CHUNK_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## 3. Executing queries via the RAG-aware agent\n",
    "- Initialize an agent with a list of tools including the built-in RAG tool. The RAG tool specification must include a list of document collection IDs to retrieve from.\n",
    "- For each prompt, initialize a new agent session, execute a turn during which a retrieval call may be requested, and output the reply received from the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95b9baa2-4739-426a-b79a-2ff90f44c023",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User> How to install OpenShift?\n",
      "{\"type\": \"function\", \"name\": \"knowledge_search\", \"parameters\": {\"query\": \"installing OpenShift\"}}Tool:knowledge_search Args:{'query': 'installing OpenShift'}Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text='Result 1:\\nDocument_id:num-0\\nContent:  We\\nrecommend you to check the official Red Hat OpenShift Local documentation for an updated list of\\nrequirements at the official documentation website.\\n\\uf05a\\nRegarding Linux, even if Red Hat does not officially support them, OpenShift Local\\ncan run on other distributions, such as Ubuntu or Debian, with minor caveats.\\nRunning OpenShift Local on any Linux distribution requires a few additional\\nsoftware packages to be installed through your default package manager. The\\n15\\ndocumentation at crc.dev/crc has more information about this subject.\\n7.2. Hardware Requirements\\nIn terms of hardware, OpenShift Local has some strict requirements. Your system must use a recent\\nIntel CPU (except for Macs, where Apple Silicon machines are supported) with at least four physical\\ncores and have at least 16 GB of RAM. Be aware that the base installation of OpenShift Local\\nrequires at least 9 GB free to start. Of course, to run other applications on OpenShift Local, you will\\nneed more RAM, so using a computer with at least 32 GB of RAM is strongly recommended.\\nOpenShift Local also requires at least 35 GB of free disk space for its installation. The memory\\nrequirements are likely to increase in the future, so please check the documentation at crc.dev for\\nmore up-to-date information.\\n7.3. Installation\\nTo install OpenShift Local, open your web browser and navigate to console.redhat.com/openshift/\\ncreate/local . Download the latest release of OpenShift Local and the \"pull secret\" file. The latter is a\\nfile containing a key identifying your copy of OpenShift Local to your Red Hat Developer account.\\nUnzip the file containing the OpenShift Local executable, and using your terminal, run the\\ncommand crc setup . This command will prepare your copy of OpenShift Local, verifying\\nrequirements and setting the required configuration values.\\nOnce the crc setup command is ready, launch crc start. Running crc start can take a long time,\\naround 20 minutes, on a recent PC.\\nOnce started, access the OpenShift Web Console with the crc console command, which will open\\nyour default browser. OpenShift Local uses the developer username and password to log in as a\\nlow-privilege user, while the kubeadmin user uses a random-generated password. Use the crc\\nconsole --credentials command to find the credentials required to log in as the kubeadmin user.\\nOpenShift Local allows developers to perform various everyday tasks as if it were a standard\\nOpenShift cluster, like deploying applications\\n', type='text'), TextContentItem(text='Result 2:\\nDocument_id:num-0\\nContent: .\\nThese characteristics set OpenShift apart as an excellent Kubernetes platform for enterprise users.\\nThe latest version of OpenShift available at the time of this writing is 4.12.\\n3.2. Is Red Hat OpenShift Open Source?\\nRed Hat OpenShift is a commercial product based on an open-source project called OKD. This\\nacronym means \" OpenShift Kubernetes Distribution\" and is publicly available for everyone to\\ninspect and contribute. Like the upstream Kubernetes project, OKD developers use the Go\\nprogramming language.\\n3.3. How can I run OpenShift?\\nToday, Red Hat OpenShift is available through various mechanisms and formats:\\n• DevOps teams can install it in their data centers \"on-premise.\"\\n• Major hyperscalers such as AWS, Azure, Google Cloud Platform, and IBM Cloud offer managed\\nRed Hat OpenShift installations.\\n• Developers can either run OpenShift locally on their workstations using Red Hat OpenShift\\nLocal, also known as CRC or \"Code-Ready Containers\"\\n• They can also request a 30-day trial OpenShift cluster, offered by Red Hat, at no charge, for\\ntesting and evaluation purposes.\\nRed Hat OpenShift is an integrated Platform-as-a-Service for enterprise users based on Kubernetes.\\nIt is tightly integrated with advanced security settings, developer tooling, and monitoring\\nmechanisms, allowing DevOps teams to be more productive.\\n8\\nChapter 4. OpenShift-only Custom Resource\\nDefinitions\\nRed Hat OpenShift is a complete DevOps platform extending Kubernetes in various ways. It bundles\\na constellation of Custom Resource Definitions (CRDs) to make the life of developers and cluster\\nadministrators easier.\\nLet us talk first about the CRDs only available on OpenShift.\\n4.1. Project\\nAn OpenShift Project is similar to a Kubernetes namespace, but more tightly integrated into the\\nsecurity system of OpenShift through additional annotations.\\napiVersion: project.openshift.io/v1\\nkind: Project\\nmetadata:\\n\\xa0 name: linkedin-learning-project\\n\\xa0 annotations:\\n\\xa0   openshift.io/description: \"Project description\"\\n\\xa0   openshift.io/display-name: \"Display name\"\\n4.2. Route\\nThe OpenShift Route object was one of the primary inspirations during the development of the\\nIngress object. In OpenShift, Ingress and Route objects work together to ensure your applications\\nare available outside the cluster.\\napiVersion: route.openshift.io/v1\\nkind: Route\\nmetadata:\\n\\xa0 name: my-route\\nspec:\\n\\xa0 host:\\n', type='text'), TextContentItem(text='Result 3:\\nDocument_id:num-0\\nContent:  \"Import from Git\" entry. Click on it, and paste the URL of a project, for example,\\ngitlab.com/akosma/simple-deno-api.git.\\nAs soon as you paste the URL, OpenShift will immediately analyze the structure and programming\\nlanguage of the project and automatically recommend options for its build process. In our case, it’s\\na small application built with the Go programming language, and as such, it will advise the options\\nshown on the screen.\\nFigure 5. Deploying a project directly from its Git repository\\n25\\nThis particular example doesn’t require more configurations than the ones shown on the screen;\\nclick the [\\u2009Create\\u2009] button.\\nAfter a few seconds, you will see your application running on the \"Topology\" screen. OpenShift will\\ndownload the source code and trigger your project’s build. Click on the Topology screen icon to see\\nthe \"Build\" section, indicating that a build is running. The compilation and deployment of your\\napplication can take some time, depending on the complexity of the source code and the\\nprogramming language used.\\nOnce the build has finished, on the same pane, you will see a route available under the \"Routes\"\\nsection. Click on it, and you will see your application in action.\\n10.2. Container Registry\\nOpenShift has built your application source code, and the product of this build process is a\\ncontainer. You can see the container that OpenShift made for you on the \"Administrator\"\\nperspective, selecting the \"Builds\" menu and then the \"ImageStreams\" menu entry.\\nOpenShift includes a container registry; developers can use it as any other registry from outside the\\ncluster. Let us use \"podman\" to access the container registry and run the container locally on your\\nworkstation.\\nUsers must have the \"registry-editor\" and the \"system:image-builder\" roles to access the container\\nregistry. Since we’re connected to the Web Console using the \"kubeadmin\" user, we can provide\\nthose roles directly from the user interface without using the command line.\\nNavigate to the \"User Management\" section and select \"RoleBindings.\" Click on the [\\u2009Create\\nbinding\\u2009] button, and fill the form using the following values:\\n• Name: developer-sourcecode-registry-editor\\n• Namespace: sourcecode\\n• Role name: registry-editor\\n• Subject: User\\n• Subject name: developer\\nDo the same for the \"system:image-builder\" role, using a different \"Name\" field\\n', type='text'), TextContentItem(text='Result 4:\\nDocument_id:num-0\\nContent: 23\\ninstall OpenShift Local, 16\\nJ\\nJAR file, 23\\nJava, 14, 24, 44\\nJavaScript, 24, 44\\nJenkins, 28\\nK\\nKiali, 36\\nKibana, 40, 40\\nKnative, 34\\nKubernetes, 7\\nL\\nlogs, 40\\nM\\nMicroservices, 36\\nmonitor, 40\\nN\\nNode.js, 14\\nnon-root accounts, 20\\nO\\nOpenShift 4.12, 33\\nOpenShift Kubernetes Distribution, 8\\nOpenShift Service Mesh, 36\\noperator, 28, 36\\nOperatorHub, 33, 36\\nOperators, 33\\n48\\nP\\nperspectives, 22\\nPHP, 14, 24\\nPlatform-as-a-Service, 8\\nprivilege escalation, 19\\nprivileged ports, 20\\nProject, 9\\nPrometheus, 40, 44\\nPromQL, 45\\nPython, 14, 24, 44\\nQ\\nQuarkus, 14, 44\\nR\\nRed Hat developer account, 13\\nRed Hat OpenShift, 7\\nRed Hat OpenShift Dev Spaces, 14\\nRed Hat OpenShift Local, 8, 15\\nRed Hat OpenShift Pipelines, 28\\nRed Hat Quay, 20\\nRed Hat Universal Base Images, 19\\nrole, 19\\nRoute, 9\\nRust, 14\\nS\\nScala, 14\\nScaling, 42\\nsecure by default, 19\\nSecurity Context Constraints, 19\\nServerless, 34\\nservice mesh, 36\\nsource code project, 22\\nstateful applications, 33\\nT\\nTekton, 28\\ntemplates, 32\\nTopology, 27\\nTwelve-Factor App, 21, 40\\nTypeScript, 24\\nU\\nUBI, 19\\nV\\nVertical scaling, 42\\nVisual Studio Code, 14\\nW\\nWeb Console, 22, 40\\n49\\n', type='text'), TextContentItem(text='Result 5:\\nDocument_id:num-0\\nContent:  Git repository, for example, but not\\n22\\nlimited to GitHub, GitLab, Gitea, or other locations.\\n• Importing YAML directly or even a JAR file with a Java application.\\nLet us select the \"Container Image\" option, where we can specify the URL of a ready-to-use\\ncontainer.\\nEnter the URL of the container on the field, and click on the [\\u2009Create\\u2009] button at the bottom of the\\npage. You do not need to change any other value on the form.\\nA few seconds later, depending on the size of the container and the speed of your Internet\\nconnection, OpenShift will have pulled the container and deployed it onto your cluster. This\\ndeployment will include the usual standard elements: a \"Deployment\" object, a \"Service\" object, and\\na \"Route.\"\\nOpenShift offers a visual representation of the applications running on your project: click on the\\nicon of your container, and you will see a panel opening on the right side of the screen. This panel\\nwill include the URL automatically assigned to your deployment, and clicking it will show the\\napplication in action in another browser tab.\\nFigure 4. Topology screen on Red Hat OpenShift\\n9.2. Creating and Debugging Applications with the odo\\nTool\\nWith the oc tool, Red Hat provides another one geared toward software developers: the odo tool.\\nDevelopers can use the odo tool to create applications using \"Devfiles,\" particular files named\\n\"devfile.yaml\" based on an open standard available at the Devfiles website. Devfiles contain\\ninformation about your application’s programming language, dependencies, and other essential\\ndetails.\\n23\\nThe odo tool is not available by default on your command line, but you can download it from the\\n\"Help\" menu on the OpenShift Web Console through the \"Command line tools\" entry. Click on the\\n\"Download odo\" link at the bottom, and select the version of odo that corresponds to your system.\\nThe \"odo catalog list components was\" command shows the various programming languages and\\nframeworks supported off-the-box by \"odo.\"\\nThe odo init  command prompts the user for a new application using many programming\\nlanguages: .NET, Go, Java, JavaScript, PHP, Python, and TypeScript. The last command generates a\\nscaffold ready to be populated with the required logic. Finally, the odo push command builds and\\npushes the container to the OpenShift container registry, deploying\\n', type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"installing OpenShift\". Use them as supporting information only in answering the question. Do not comment on them directly. Your reply should only answer the query.\\n', type='text')]To install and use Red Hat OpenShift, you can follow these steps:\n",
      "\n",
      "1. **Create a Red Hat developer account**: Go to the Red Hat website and sign up for a free developer account.\n",
      "2. **Download the oc tool**: The oc (OpenShift CLI) is the command-line interface used to interact with your OpenShift cluster. You can download it from the \"Help\" menu on the OpenShift Web Console through the \"Command line tools\" entry.\n",
      "3. **Install Red Hat OpenShift Local**: Red Hat OpenShift Local is a self-contained, single-node version of OpenShift that you can run on your local machine. You can download and install it from the Red Hat website.\n",
      "4. **Create an OpenShift cluster**: You can create a new OpenShift cluster using the oc tool or by importing an existing cluster.\n",
      "5. **Import a Git repository or container image**: You can import a Git repository, such as one from GitHub, GitLab, or Gitea, or import a container image directly into your OpenShift project.\n",
      "6. **Use the odo tool to create and debug applications**: The odo tool is used to create applications using \"Devfiles,\" which contain information about your application's programming language, dependencies, and other essential details.\n",
      "\n",
      "Alternatively, you can also use the following methods:\n",
      "\n",
      "* Importing YAML directly or a JAR file with a Java application.\n",
      "* Creating a new project from scratch using the OpenShift Web Console.\n",
      "* Using Red Hat Quay to manage container images.\n",
      "\n",
      "Note: The specific steps may vary depending on your environment and requirements.User> Are employees based in California eligible for remote work?\n",
      "{\"type\": \"function\", \"name\": \"knowledge_search\", \"parameters\": {\"query\": \"California remote work policy for employees\"}}Tool:knowledge_search Args:{'query': 'California remote work policy for employees'}Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text='Result 1:\\nDocument_id:num-1\\nContent:  Set clear performance expectations and key performance indicators (KPIs) \\n     for remote employees.\\n      • Implement regular performance reviews and feedback sessions, even if they\\n      are conducted virtually.\\n      • Use performance management software and tools to track and measure \\n      remote employees’ contributions and achievements.\\nConclusion\\nBy addressing these five issues and implementing the associated compliance\\nstrategies, California employers can successfully manage their remote workforce,\\nmitigate legal risks, and create a legally-compliant remote work environment that\\nbenefits both the employer and the remote workers. Continually adapting and\\nrefining remote work policies and practices is the key to staying in compliance with\\nCalifornia’s labor and employment laws and to meeting the evolving needs of the\\nworkforce.\\nFor questions regarding complying with California employment-related remote\\nwork laws, please contact either author - Alessandra Whipple at\\nawhipple@cdflaborlaw.com or Todd Wulffson at twulffson@cdflaborlaw.com. \\nBy Alessandra C. Whipple, Esq. and Todd R. Wulffson, Esq.\\nWhipple Wulffson\\n\\n', type='text'), TextContentItem(text='Result 2:\\nDocument_id:num-1\\nContent: B-38 ORANGE COUNTY BUSINESS JOURNAL                                                                                              GENERAL COUNSEL AWARDS OCTOBER 9, 2023\\nDuring and since the pandemic, the traditional office commute rapidly evolved\\nfrom the familiar dystopia of So. California freeways, to the more comfortable\\nmorning sweatpants shuffle down the hall to a desk or couch. As remote work\\nappears to be lingering, and for some, may be here to stay, employers –\\nespecially in California (infamous for its complex and ever-changing labor and\\nemployment laws), face unique challenges and concerns. In this article, we will\\nexplore the top five issues facing California employers\\nregarding remote work and offer strategies for\\ncompliance.\\n1. Wage and Hour Compliance\\nOne of the foremost concerns for California employers\\nwith all employees, but especially with remote workers,\\nis ensuring compliance with wage and hour laws.\\nCalifornia has a complex web of regulations governing\\novertime, meal and rest breaks, and record-keeping. If\\nan employee performs services in California (remote\\nworkers in other states cannot take advantage of\\nCalifornia law), employers must track the hours worked\\nby remote employees accurately, regardless of their location, and pay them in\\naccordance with California’s labor laws.\\nCompliance Strategies\\n      • Implement time-tracking software to monitor remote employees’ work hours.\\n      • Train managers and employees on timekeeping best practices and the \\n      importance of accurate record-keeping.\\n      • Develop clear remote work policies (preferably that remote workers sign), \\n      that outline expectations regarding work hours and reporting time worked.\\n• These policies should clearly state that all off-the-clock work is strictly \\nprohibited. \\n• Have employees confirm in writing that they have accurately reported all \\nof their time worked.\\n• Prohibit texting, e-mailing, and phone calls when employees are not \\nclocked-in and educate and restrict managers from contacting hourly \\nemployees after hours.\\n      • Develop and enforce meal and rest break policies.\\n• Consider developing a break schedule that is circulated to managers and \\nremote workers, which will mitigate potential unintended meal   \\ninterruptions. \\n• Meal and rest break policies should clearly state that employees are \\nprohibited from performing work while on break.\\n• Employees should inform remote employees to report missed meal or rest \\nbreaks, and to have employees regularly certify (quarterly or monthly) that \\nthey have been provided the opportunity to take their meal and rest \\nbreaks.\\nThese strategies will go a long way toward mitig\\n', type='text'), TextContentItem(text='Result 3:\\nDocument_id:num-1\\nContent:  reimbursement policy and procedures.\\n      • Keep detailed records of reimbursed expenses and any accommodation \\n      related correspondence.\\n3. Data Security and Privacy\\nRemote work can pose significant challenges regarding\\ndata security and privacy. California has strong data\\nprotection laws, such as the California Consumer\\nPrivacy Act (CCPA) and the California Privacy Rights\\nAct (CPRA). Employers must safeguard sensitive\\ninformation and comply with these regulations, which\\napply regardless of where the employee works.\\nCompliance Strategies\\n• Ensure that remote employees have access to\\nsecure technology and resources to protect \\ncompany data.\\n• Implement strict data security policies, \\nincluding encryption, secure file sharing, and regular cybersecurity training.\\n• Implement policies regarding the destruction and \\n      return of company data and property following separation of employment.\\n      • Seek legal counsel to navigate California’s complex and evolving data \\n      privacy landscape.\\n4. Workers’ Compensation and Workplace Safety\\nEnsuring the safety and well-being of remote workers is a significant concern for\\nCalifornia employers. While traditional workplace safety regulations primarily apply\\nto physical office spaces, employers are still responsible for providing a safe work\\nenvironment for remote employees.\\nCompliance Strategies\\n      • Advise workers’ compensation insurance carriers that the company has \\n     remote employees to ensure coverage is not jeopardized when an injury \\n      occurs to a remote worker.\\n      • Develop a remote work safety policy that outlines best practices for \\n      maintaining a safe remote work environment.\\n      • Understand the nuances of workers’ compensation laws as they relate to \\n      remote work and ensure appropriate coverage.\\n5. Performance Management and Evaluation\\nEvaluating and managing the performance of remote employees can be more\\nchallenging than for in-office employees. Employers must find effective ways to\\nassess productivity, provide feedback, and conduct performance evaluations, all\\nwhile adhering to California labor laws.\\nCompliance Strategies\\n      • Set clear performance expectations and key performance indicators (KPIs) \\n     for remote employees.\\n      • Implement regular performance reviews and feedback sessions, even if they\\n      are conducted virtually.\\n      • Use performance management software and tools to track and measure \\n      remote employees’ contributions and achievements.\\nConclusion\\nBy addressing these five issues and implementing the associated compliance\\nstrategies, California employers can successfully manage their remote workforce,\\nmitigate legal risks, and create a legally-compliant remote work environment that\\nbenefits both the employer and the remote workers. Continually adapting and\\nrefining remote work policies and practices is the key to staying in\\n', type='text'), TextContentItem(text='Result 4:\\nDocument_id:num-1\\nContent:  and restrict managers from contacting hourly \\nemployees after hours.\\n      • Develop and enforce meal and rest break policies.\\n• Consider developing a break schedule that is circulated to managers and \\nremote workers, which will mitigate potential unintended meal   \\ninterruptions. \\n• Meal and rest break policies should clearly state that employees are \\nprohibited from performing work while on break.\\n• Employees should inform remote employees to report missed meal or rest \\nbreaks, and to have employees regularly certify (quarterly or monthly) that \\nthey have been provided the opportunity to take their meal and rest \\nbreaks.\\nThese strategies will go a long way toward mitigating expensive, distracting, and\\ninfuriating lawsuits - particularly class and PAGA actions. \\n2. Expense Reimbursement\\nCalifornia Labor Code Section 2802 requires employers to reimburse employees\\nfor “all necessary business expenditures or losses incurred by the employee in\\ndirect consequence of the discharge of his or her duties.” For remote workers, this\\nmay include internet usage, home electricity, computer equipment, and cell phone\\nusage. Employers should provide employees with the necessary supplies and\\nequipment needed to perform their job remotely, and either (1) make employees\\naware of the procedure for expensing incidental expenses, or (2) provide a\\nmonthly stipend for incidental expenses like cell phone, electricity and internet at\\nhome ($25 is a good amount). Make sure you do not have any antiquated policies\\nthat say home expenses are not reimbursable. If an employee chooses to work\\nfrom home, they may not have the same rights to reimbursement as an employee\\nwhose home is their assigned workspace.\\nCompliance Strategies\\n      • Establish a comprehensive expense reimbursement policy that outlines \\n      eligible expenses and the reimbursement process.\\nNavigating Remote Work Challenges: Top Five Issues for California Employers\\n      • Maintain clear communication with remote employees about the \\n      reimbursement policy and procedures.\\n      • Keep detailed records of reimbursed expenses and any accommodation \\n      related correspondence.\\n3. Data Security and Privacy\\nRemote work can pose significant challenges regarding\\ndata security and privacy. California has strong data\\nprotection laws, such as the California Consumer\\nPrivacy Act (CCPA) and the California Privacy Rights\\nAct (CPRA). Employers must safeguard sensitive\\ninformation and comply with these regulations, which\\napply regardless of where the employee works.\\nCompliance Strategies\\n• Ensure that remote employees have access to\\nsecure technology and resources to protect \\ncompany data.\\n• Implement strict data security policies, \\n\\n', type='text'), TextContentItem(text='Result 5:\\nDocument_id:num-0\\nContent: .\\nTekton offers EventListener and Trigger objects that can be called from outside the cluster, for\\nexample, from GitHub or GitLab, so that each time your source code changes, a new PipelineRun\\nstarts, and you could even request a redeployment of your code in production.\\nHowever, if you use CRC, the trigger URL is a subdomain of the \"apps-crc.testing\" domain, only\\naccessible from your local machine. There are ways to expose this URL using a reverse proxy such\\nas ngrok, but such a task is outside of the scope of this course.\\nAlso of interest, Tekton offers a Visual Studio Code extension to help developers create the YAML\\nrepresentations of Workspaces, Tasks, Pipelines, and other objects.\\n11.3. Inspecting the Pipeline\\nClick on the \"Pipelines\" entry on the left-hand side menu of the OpenShift web console. This screen\\ndisplays the list of Tekton pipelines defined in the cluster at any time. It also conveniently shows\\nthe status of the pipeline; a full green bar means, of course, that the last execution of the pipeline\\nwas a success.\\nSelect the item on the list with the same name as the application created previously. The \"Pipeline\\n29\\ndetails\" screen shows the pipeline structure, which is precisely the same as seen once during the\\ndeployment creation.\\nThe tab \"PipelineRuns\" contains the history of all the times when the pipeline was executed,\\nincluding its status and result. Select the last item on that list and click on any pipeline steps. A new\\npane will show the logs of the execution of the pipeline, including all the operational records.\\n\\uf0eb The branch 02_06 of the GitHub repository for this course contains an example of a\\nCI/CD pipeline built with YAML files.\\n30\\nAdvanced Cloud Native Apps\\nIn this section you will learn some advanced topics, like templates, operators, serverless with\\nKnative, and service mesh with Istio.\\n31\\nChapter 12. Templates and Operators\\nLet’s talk about OpenShift 4.12 templates and operators, both powerful ways to create and deploy\\napplications on OpenShift.\\n12.1. Templates\\nOpenShift templates are YAML or JSON files that describe the desired state of application\\ncomponents, such as pods, services, routes, and build configurations. You can use templates to\\ndefine reusable and parameterized application configurations that can be instantiated with a single\\ncommand or through the web console.\\nTemplates can help you simplify\\n', type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"California remote work policy for employees\". Use them as supporting information only in answering the question. Do not comment on them directly. Your reply should only answer the query.\\n', type='text')]Based on the provided text, here is a summary of the top five issues for California employers related to remote work policies:\n",
      "\n",
      "1. **Expense Reimbursement**: Employers must reimburse employees for necessary business expenditures or losses incurred in direct consequence of their duties, including internet usage, home electricity, computer equipment, and cell phone usage.\n",
      "2. **Data Security and Privacy**: Employers must safeguard sensitive information and comply with California's strong data protection laws, such as the California Consumer Privacy Act (CCPA) and the California Privacy Rights Act (CPRA).\n",
      "3. **Meal and Rest Breaks**: Employees working from home may be entitled to meal and rest breaks, which employers must provide in accordance with California law.\n",
      "4. **Expense Reimbursement Policy**: Employers should establish a comprehensive expense reimbursement policy that outlines eligible expenses and the reimbursement process.\n",
      "5. **Class Action Lawsuits**: Employers may face class action lawsuits related to remote work policies, particularly if they fail to comply with California labor laws.\n",
      "\n",
      "It's essential for California employers to develop clear and compliant remote work policies to avoid these issues and ensure a positive experience for both employees and management."
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"How to install OpenShift?\",\n",
    "    \"Are employees based in California eligible for remote work?\",\n",
    "]\n",
    "\n",
    "# initializing the agent\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=MODEL_ID,\n",
    "    instructions=SYSTEM_PROMPT,\n",
    "    sampling_params={\n",
    "        \"strategy\": strategy,\n",
    "    },\n",
    "    # we make our agent aware of the RAG tool by including builtin::rag/knowledge_search in the list of tools\n",
    "    tools=[\n",
    "        dict(\n",
    "            name=\"builtin::rag/knowledge_search\",\n",
    "            args={\n",
    "                \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "for prompt in queries:\n",
    "    print(f\"User> {prompt}\")\n",
    "    \n",
    "    # create a new turn with a new session ID for each prompt\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=agent.create_session(f\"rag-session_{uuid.uuid4()}\")\n",
    "    )\n",
    "    \n",
    "    # print the response, including tool calls output\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        print(log.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6937a3-3efa-4b66-aaf0-85d96b6d43db",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "This tutorial demonstrates how to implement agentic RAG with Llama Stack. We do so by initializing an agent while giving it access to the RAG tool, then invoking the agent on each of the specified queries. Please check out our [complementary tutorial](demos/rag_agentic/notebooks/Level1_foundational_RAG.ipynb) for a non-agentic RAG example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
