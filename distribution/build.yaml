version: '2'
distribution_spec:
  description: Use (an external) vLLM server for running LLM inference
  providers:
    inference:
    - provider_type: remote::vllm
    - provider_type: inline::sentence-transformers
    vector_io:
    - provider_type: inline::milvus
    safety:
    - provider_type: inline::llama-guard
    agents:
    - provider_type: inline::meta-reference
    eval:
    - provider_type: inline::meta-reference
    datasetio:
    - provider_type: remote::huggingface
    - provider_type: inline::localfs
    scoring:
    - provider_type: inline::basic
    - provider_type: inline::llm-as-judge
    - provider_type: inline::braintrust
    telemetry:
    - provider_type: inline::meta-reference
    tool_runtime:
    - provider_type: remote::brave-search
    - provider_type: remote::tavily-search
    - provider_type: inline::rag-runtime
    - provider_type: remote::model-context-protocol
  container_image: "registry.access.redhat.com/ubi9"
image_type: container
image_name: llama-stack-demos
