{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Goal\n",
    "The overall goal of the experiment is to evaluate the tool calling limitations of small LLMs (1B - 3B params) and find ways (through prompting, tool descriptions, etc.) to increase performance."
   ],
   "id": "f02eceeb0d3f3235"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This first cell shows giving `llama3.2:3B` 32 tools and then stores metrics of the accuracy levels. 32 is set as the `TOOL_LIMIT` because any increase leads to complete degradation in accuracy. That is one of the most surprising results of this experiment because intuitively one would expect a decrease in accuracy but not a complete drop.",
   "id": "21863d62bc6413e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import utils\n",
    "from llama_stack_client.lib.agents.client_tool import ClientTool\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from tools import tools\n",
    "\n",
    "# Max client tools llama3.2:3B can handle\n",
    "TOOL_LIMIT = 32\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def load_queries(file_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Load query strings from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        if not isinstance(data, dict) or 'queries' not in data:\n",
    "            raise ValueError(f\"Invalid JSON format in {file_path}\")\n",
    "\n",
    "        # Return full query objects with ID for better test identification\n",
    "        return data['queries']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Query file not found: {file_path}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def get_query_id(query_obj):\n",
    "    \"\"\"Extract an ID from a query object for better test identification.\"\"\"\n",
    "    if isinstance(query_obj, dict) and 'id' in query_obj:\n",
    "        return query_obj['id']\n",
    "    elif isinstance(query_obj, dict) and 'query' in query_obj:\n",
    "        # Use first few words of query if no ID is available\n",
    "        words = query_obj['query'].split()[:5]\n",
    "        return '_'.join(words).lower().replace(',', '').replace('.', '')\n",
    "    return \"unknown_query\"\n",
    "\n",
    "def execute_query(\n",
    "    client: LlamaStackClient,\n",
    "    prompt: str,\n",
    "    model: str,\n",
    "    tools: Union[List[str], List[Any]], # list of toolgroup_ids or tool objects\n",
    "    instructions: Optional[str] = None,\n",
    "    max_tokens: int = 4096\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute a single query with a given set of tools.\"\"\"\n",
    "\n",
    "    if instructions is None:\n",
    "        # Default instructions for general tool use\n",
    "        instructions = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "            Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\n",
    "            When you are asked to search the web you must use a tool. Keep answers concise.\n",
    "            \"\"\"\n",
    "\n",
    "    # Create agent with tools\n",
    "    agent = Agent(\n",
    "        client,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=tools,\n",
    "        tool_config={\"tool_choice\": \"auto\"},\n",
    "        sampling_params={\"max_tokens\": max_tokens}\n",
    "    )\n",
    "\n",
    "    # Create session\n",
    "    session_id = agent.create_session(session_name=f\"Test_{int(time.time())}\")\n",
    "    print(f\"Created session_id={session_id} for Agent({agent.agent_id})\" if not all(isinstance(t, str) for t in tools) else \"\")\n",
    "\n",
    "    turn_response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    return turn_response\n",
    "\n",
    "def run_client_tool_test(model, query_obj, client_tool_module, llama_client, logger):\n",
    "    \"\"\"Run a single test for a specific model and query.\"\"\"\n",
    "    query_id = get_query_id(query_obj)\n",
    "    prompt = query_obj['query']\n",
    "    expected_tool_call = query_obj['tool_call']\n",
    "\n",
    "    tool_list = []\n",
    "    for name in dir(client_tool_module):\n",
    "        if len(tool_list) >= TOOL_LIMIT:\n",
    "            break\n",
    "        attribute = getattr(client_tool_module, name)\n",
    "        if isinstance(attribute, ClientTool):\n",
    "            tool_list.append(attribute)\n",
    "\n",
    "    logger.info(f\"Testing query '{query_id}' with model {model}\")\n",
    "    logger.info(f\"Query: {prompt[:50]}...\")\n",
    "\n",
    "    try:\n",
    "        response = execute_query(\n",
    "            client=llama_client,\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            tools=tool_list,\n",
    "        )\n",
    "        # Get Tool execution and Inference steps\n",
    "        steps = response.steps\n",
    "\n",
    "        #Get tool used\n",
    "        try:\n",
    "            tools_used = steps[1].tool_calls[0].tool_name\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting tool name: {e}\")\n",
    "            tools_used = None\n",
    "        tool_call_match = True if tools_used == expected_tool_call else False\n",
    "        logger.info(f\"Tool used: {tools_used} Tool expected: {expected_tool_call} match: {tool_call_match} \")\n",
    "\n",
    "        #Check inference was not empty\n",
    "        final_response = \"\"\n",
    "        try:\n",
    "            final_response = steps[2].api_model_response.content.strip()\n",
    "            inference_not_empty = True if final_response != '' else False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking inference content: {e}\")\n",
    "            inference_not_empty = False\n",
    "        logger.info(f'Inference not empty: {inference_not_empty}')\n",
    "        logger.info(f\"Query '{query_id}' succeeded with model {model} and the response \\n {final_response}\")\n",
    "\n",
    "        # Record success metrics, including the expected_tool_call\n",
    "        utils.add_metric(\n",
    "            model=model,\n",
    "            query_id=query_id,\n",
    "            status=\"SUCCESS\",\n",
    "            tool_call_match=tool_call_match,\n",
    "            inference_not_empty=inference_not_empty,\n",
    "            expected_tool_call=expected_tool_call\n",
    "        )\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        logger.error(f\"Query '{query_id}' failed with model {model}: {error_msg}\")\n",
    "\n",
    "        # Record failure metrics\n",
    "        utils.add_metric(\n",
    "            model=model,\n",
    "            query_id=query_id,\n",
    "            status=\"FAILURE\",\n",
    "            tool_call_match=False,\n",
    "            inference_not_empty=False,\n",
    "            expected_tool_call=expected_tool_call,\n",
    "            error=error_msg\n",
    "        )\n",
    "\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run all tests.\"\"\"\n",
    "    # Set up logger\n",
    "    logger = utils.setup_logger()\n",
    "\n",
    "    # Create client\n",
    "    base_url = os.getenv('REMOTE_BASE_URL')\n",
    "    print(f\"base_url={base_url}\")\n",
    "    if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        return\n",
    "\n",
    "    llama_client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "    # Define models to test\n",
    "    # make sure they are available in your LLS server\n",
    "    models = [\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "\n",
    "    client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries.json\")\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        if client_tool_queries:\n",
    "            queries = load_queries(client_tool_queries)\n",
    "\n",
    "            if not queries:\n",
    "                logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "                continue\n",
    "\n",
    "            for query_obj in queries:\n",
    "                total_tests += 1\n",
    "                success = run_client_tool_test(model, query_obj, tools, llama_client, logger)\n",
    "                if success:\n",
    "                    successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots('./results/normal_client_tool_metrics.csv')\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![Tool Call Match Per Function Tool](results/plots/normal_tool_call_match_per_function_tool.png)\n",
    "Overall majority of the tools have 100% accuracy, the tool with the worst accuracy is `convert_fahrenheit_to_kelvin`. One possible explanation could be that the training data likely had very little, if any, data on this type of Q/A. `convert_celsius_to_kelvin` has 100% accuracy which affirms this theory as that's a common conversion in science courses."
   ],
   "id": "65757d655859540f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The next few cells run experiments to test what truly matters when defining a tool: the tool name, description, and the format of the docstring. A quick overview of how `llama-stack` parses the function when tagged with the `client_tool` decorator.\n",
    "\n",
    "```python\n",
    "def client_tool(func: T) -> ClientTool:\n",
    "    \"\"\"\n",
    "    Decorator to convert a function into a ClientTool.\n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "    class _WrappedTool(ClientTool):\n",
    "        __name__ = func.__name__\n",
    "        __doc__ = func.__doc__\n",
    "        __module__ = func.__module__\n",
    "\n",
    "        def get_name(self) -> str:\n",
    "            ...\n",
    "\n",
    "        def get_description(self) -> str:\n",
    "            ...\n",
    "\n",
    "        def get_params_definition(self) -> Dict[str, Parameter]:\n",
    "            hints = get_type_hints(func)\n",
    "            # Remove return annotation if present\n",
    "            hints.pop(\"return\", None)\n",
    "\n",
    "            # Get parameter descriptions from docstring\n",
    "            params = {}\n",
    "            sig = inspect.signature(func)\n",
    "            doc = inspect.getdoc(func) or \"\"\n",
    "\n",
    "            for name, type_hint in hints.items():\n",
    "                # Look for :param name: in docstring\n",
    "                param_doc = \"\"\n",
    "                for line in doc.split(\"\\n\"):\n",
    "                    if line.strip().startswith(f\":param {name}:\"):\n",
    "                        param_doc = line.split(\":\", 2)[2].strip()\n",
    "                        break\n",
    "\n",
    "                if param_doc == \"\":\n",
    "                    raise ValueError(f\"No parameter description found for parameter {name}\")\n",
    "\n",
    "                ...\n",
    "\n",
    "            return params\n",
    "```\n",
    "Full implementation can be found [here](https://github.com/meta-llama/llama-stack-client-python/blob/645d2195c5af1c6f903cb93c293319d8f94c36cc/src/llama_stack_client/lib/agents/client_tool.py#L150-L170).\n",
    "An important thing to realize is that `llama-stack` **purposefully** disregards the return information from the docstring. Also that the docstring only **requires** one annotation, `:params:`, and everything _above_ that will be parsed as well."
   ],
   "id": "765e19f8bba53e6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This next cell will test whether explicitly having `:description:` and `:use_case:` annotations help, compared to including them without any annotation.\n",
    "\n",
    "Ex.\n",
    "```python\n",
    "@client_tool\n",
    "def add_two_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    :description: Adds two numbers.\n",
    "    :use_case: Use when the user wants to find the sum, total, or combined value of two numbers.\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    :returns: The sum of `a` and `b`.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```\n",
    "\n",
    "compared to\n",
    "\n",
    "```python\n",
    "@client_tool\n",
    "def add_two_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Adds two numbers.\n",
    "    Use when the user wants to find the sum, total, or combined value of two numbers.\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    :returns: The sum of `a` and `b`.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```"
   ],
   "id": "466e2edfe4fdf8fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import utils\n",
    "from llama_stack_client.lib.agents.client_tool import ClientTool\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from tools import tools_no_extra_tags\n",
    "\n",
    "# Max client tools llama3.2:3B can handle\n",
    "TOOL_LIMIT = 32\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def load_queries(file_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Load query strings from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        if not isinstance(data, dict) or 'queries' not in data:\n",
    "            raise ValueError(f\"Invalid JSON format in {file_path}\")\n",
    "\n",
    "        # Return full query objects with ID for better test identification\n",
    "        return data['queries']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Query file not found: {file_path}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def get_query_id(query_obj):\n",
    "    \"\"\"Extract an ID from a query object for better test identification.\"\"\"\n",
    "    if isinstance(query_obj, dict) and 'id' in query_obj:\n",
    "        return query_obj['id']\n",
    "    elif isinstance(query_obj, dict) and 'query' in query_obj:\n",
    "        # Use first few words of query if no ID is available\n",
    "        words = query_obj['query'].split()[:5]\n",
    "        return '_'.join(words).lower().replace(',', '').replace('.', '')\n",
    "    return \"unknown_query\"\n",
    "\n",
    "def execute_query(\n",
    "    client: LlamaStackClient,\n",
    "    prompt: str,\n",
    "    model: str,\n",
    "    tools: Union[List[str], List[Any]], # list of toolgroup_ids or tool objects\n",
    "    instructions: Optional[str] = None,\n",
    "    max_tokens: int = 4096\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute a single query with a given set of tools.\"\"\"\n",
    "\n",
    "    if instructions is None:\n",
    "        # Default instructions for general tool use\n",
    "        instructions = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "            Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\n",
    "            When you are asked to search the web you must use a tool. Keep answers concise.\n",
    "            \"\"\"\n",
    "\n",
    "    # Create agent with tools\n",
    "    agent = Agent(\n",
    "        client,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=tools,\n",
    "        tool_config={\"tool_choice\": \"auto\"},\n",
    "        sampling_params={\"max_tokens\": max_tokens}\n",
    "    )\n",
    "\n",
    "    # Create session\n",
    "    session_id = agent.create_session(session_name=f\"Test_{int(time.time())}\")\n",
    "    print(f\"Created session_id={session_id} for Agent({agent.agent_id})\" if not all(isinstance(t, str) for t in tools) else \"\")\n",
    "\n",
    "    turn_response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    return turn_response\n",
    "\n",
    "def run_client_tool_test(model, query_obj, client_tool_module, llama_client, logger):\n",
    "    \"\"\"Run a single test for a specific model and query.\"\"\"\n",
    "    query_id = get_query_id(query_obj)\n",
    "    prompt = query_obj['query']\n",
    "    expected_tool_call = query_obj['tool_call']\n",
    "\n",
    "    tool_list = []\n",
    "    for name in dir(client_tool_module):\n",
    "        if len(tool_list) >= TOOL_LIMIT:\n",
    "            break\n",
    "        attribute = getattr(client_tool_module, name)\n",
    "        if isinstance(attribute, ClientTool):\n",
    "            tool_list.append(attribute)\n",
    "\n",
    "    logger.info(f\"Testing query '{query_id}' with model {model}\")\n",
    "    logger.info(f\"Query: {prompt[:50]}...\")\n",
    "\n",
    "    try:\n",
    "        response = execute_query(\n",
    "            client=llama_client,\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            tools=tool_list,\n",
    "        )\n",
    "        # Get Tool execution and Inference steps\n",
    "        steps = response.steps\n",
    "\n",
    "        #Get tool used\n",
    "        try:\n",
    "            tools_used = steps[1].tool_calls[0].tool_name\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting tool name: {e}\")\n",
    "            tools_used = None\n",
    "        tool_call_match = True if tools_used == expected_tool_call else False\n",
    "        logger.info(f\"Tool used: {tools_used} Tool expected: {expected_tool_call} match: {tool_call_match} \")\n",
    "\n",
    "        #Check inference was not empty\n",
    "        final_response = \"\"\n",
    "        try:\n",
    "            final_response = steps[2].api_model_response.content.strip()\n",
    "            inference_not_empty = True if final_response != '' else False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking inference content: {e}\")\n",
    "            inference_not_empty = False\n",
    "        logger.info(f'Inference not empty: {inference_not_empty}')\n",
    "        logger.info(f\"Query '{query_id}' succeeded with model {model} and the response \\n {final_response}\")\n",
    "\n",
    "        # Record success metrics, including the expected_tool_call\n",
    "        utils.add_metric(\n",
    "            model=model,\n",
    "            query_id=query_id,\n",
    "            status=\"SUCCESS\",\n",
    "            tool_call_match=tool_call_match,\n",
    "            inference_not_empty=inference_not_empty,\n",
    "            expected_tool_call=expected_tool_call\n",
    "        )\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        logger.error(f\"Query '{query_id}' failed with model {model}: {error_msg}\")\n",
    "\n",
    "        # Record failure metrics\n",
    "        utils.add_metric(\n",
    "            model=model,\n",
    "            query_id=query_id,\n",
    "            status=\"FAILURE\",\n",
    "            tool_call_match=False,\n",
    "            inference_not_empty=False,\n",
    "            expected_tool_call=expected_tool_call,\n",
    "            error=error_msg\n",
    "        )\n",
    "\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run all tests.\"\"\"\n",
    "    # Set up logger\n",
    "    logger = utils.setup_logger()\n",
    "\n",
    "    # Create client\n",
    "    base_url = os.getenv('REMOTE_BASE_URL')\n",
    "    print(f\"base_url={base_url}\")\n",
    "    if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        return\n",
    "\n",
    "    llama_client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "    # Define models to test\n",
    "    # make sure they are available in your LLS server\n",
    "    models = [\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "\n",
    "    client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries.json\")\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        if client_tool_queries:\n",
    "            queries = load_queries(client_tool_queries)\n",
    "\n",
    "            if not queries:\n",
    "                logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "                continue\n",
    "\n",
    "            for query_obj in queries:\n",
    "                total_tests += 1\n",
    "                success = run_client_tool_test(model, query_obj, tools_no_extra_tags, llama_client, logger)\n",
    "                if success:\n",
    "                    successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots('./results/no_extra_tools_client_tool_metrics.csv')\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "id": "32cc19b7d46ad666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![no_extra_tags_tool_call_match_per_function_tool.jpg](results/plots/no_extra_tags_tool_call_match_per_function_tool.jpg)\n",
    "\n",
    "Based off the results from removing the explicit `:description:` and `:use_case:` there is a clear decrease in accuracy. It is **not** a fact that adding them will increase accuracy but for our query set it helped."
   ],
   "id": "4f7fe564688ff04f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This next cell will test whether the tool name matters at all. To do this test, all functions were renamed to `function_1`, `function_2`, etc. but the docstring was left unchanged.\n",
    "\n",
    "Ex.\n",
    "```python\n",
    "@client_tool\n",
    "def function_1(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    :description: Adds two numbers.\n",
    "    :use_case: Use when the user wants to find the sum, total, or combined value of two numbers.\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    :returns: The sum of `a` and `b`.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```"
   ],
   "id": "ee5f00dc9a51ee1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import utils\n",
    "from llama_stack_client.lib.agents.client_tool import ClientTool\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from tools import tools_bad_function_names\n",
    "\n",
    "# Max client tools llama3.2:3B can handle\n",
    "TOOL_LIMIT = 32\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def load_queries(file_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Load query strings from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        if not isinstance(data, dict) or 'queries' not in data:\n",
    "            raise ValueError(f\"Invalid JSON format in {file_path}\")\n",
    "\n",
    "        # Return full query objects with ID for better test identification\n",
    "        return data['queries']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Query file not found: {file_path}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def get_query_id(query_obj):\n",
    "    \"\"\"Extract an ID from a query object for better test identification.\"\"\"\n",
    "    if isinstance(query_obj, dict) and 'id' in query_obj:\n",
    "        return query_obj['id']\n",
    "    elif isinstance(query_obj, dict) and 'query' in query_obj:\n",
    "        # Use first few words of query if no ID is available\n",
    "        words = query_obj['query'].split()[:5]\n",
    "        return '_'.join(words).lower().replace(',', '').replace('.', '')\n",
    "    return \"unknown_query\"\n",
    "\n",
    "def execute_query(\n",
    "    client: LlamaStackClient,\n",
    "    prompt: str,\n",
    "    model: str,\n",
    "    tools: Union[List[str], List[Any]], # list of toolgroup_ids or tool objects\n",
    "    instructions: Optional[str] = None,\n",
    "    max_tokens: int = 4096\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute a single query with a given set of tools.\"\"\"\n",
    "\n",
    "    if instructions is None:\n",
    "        # Default instructions for general tool use\n",
    "        instructions = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "            Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\n",
    "            When you are asked to search the web you must use a tool. Keep answers concise.\n",
    "            \"\"\"\n",
    "\n",
    "    # Create agent with tools\n",
    "    agent = Agent(\n",
    "        client,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=tools,\n",
    "        tool_config={\"tool_choice\": \"auto\"},\n",
    "        sampling_params={\"max_tokens\": max_tokens}\n",
    "    )\n",
    "\n",
    "    # Create session\n",
    "    session_id = agent.create_session(session_name=f\"Test_{int(time.time())}\")\n",
    "    print(f\"Created session_id={session_id} for Agent({agent.agent_id})\" if not all(isinstance(t, str) for t in tools) else \"\")\n",
    "\n",
    "    turn_response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    return turn_response\n",
    "\n",
    "def run_client_tool_test(model, query_obj, client_tool_module, llama_client, logger):\n",
    "    \"\"\"Run a single test for a specific model and query.\"\"\"\n",
    "    query_id = get_query_id(query_obj)\n",
    "    prompt = query_obj['query']\n",
    "    expected_tool_call = query_obj['tool_call']\n",
    "\n",
    "    tool_list = []\n",
    "    for name in dir(client_tool_module):\n",
    "        if len(tool_list) >= TOOL_LIMIT:\n",
    "            break\n",
    "        attribute = getattr(client_tool_module, name)\n",
    "        if isinstance(attribute, ClientTool):\n",
    "            tool_list.append(attribute)\n",
    "\n",
    "    logger.info(f\"Testing query '{query_id}' with model {model}\")\n",
    "    logger.info(f\"Query: {prompt[:50]}...\")\n",
    "\n",
    "    try:\n",
    "        response = execute_query(\n",
    "            client=llama_client,\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            tools=tool_list,\n",
    "        )\n",
    "        # Get Tool execution and Inference steps\n",
    "        steps = response.steps\n",
    "\n",
    "        #Get tool used\n",
    "        try:\n",
    "            tools_used = steps[1].tool_calls[0].tool_name\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting tool name: {e}\")\n",
    "            tools_used = None\n",
    "        tool_call_match = True if tools_used == expected_tool_call else False\n",
    "        logger.info(f\"Tool used: {tools_used} Tool expected: {expected_tool_call} match: {tool_call_match} \")\n",
    "\n",
    "        #Check inference was not empty\n",
    "        final_response = \"\"\n",
    "        try:\n",
    "            final_response = steps[2].api_model_response.content.strip()\n",
    "            inference_not_empty = True if final_response != '' else False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking inference content: {e}\")\n",
    "            inference_not_empty = False\n",
    "        logger.info(f'Inference not empty: {inference_not_empty}')\n",
    "        logger.info(f\"Query '{query_id}' succeeded with model {model} and the response \\n {final_response}\")\n",
    "\n",
    "        # Record success metrics, including the expected_tool_call\n",
    "        utils.add_metric(\n",
    "            model=model,\n",
    "            query_id=query_id,\n",
    "            status=\"SUCCESS\",\n",
    "            tool_call_match=tool_call_match,\n",
    "            inference_not_empty=inference_not_empty,\n",
    "            expected_tool_call=expected_tool_call\n",
    "        )\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        logger.error(f\"Query '{query_id}' failed with model {model}: {error_msg}\")\n",
    "\n",
    "        # Record failure metrics\n",
    "        utils.add_metric(\n",
    "            model=model,\n",
    "            query_id=query_id,\n",
    "            status=\"FAILURE\",\n",
    "            tool_call_match=False,\n",
    "            inference_not_empty=False,\n",
    "            expected_tool_call=expected_tool_call,\n",
    "            error=error_msg\n",
    "        )\n",
    "\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run all tests.\"\"\"\n",
    "    # Set up logger\n",
    "    logger = utils.setup_logger()\n",
    "\n",
    "    # Create client\n",
    "    base_url = os.getenv('REMOTE_BASE_URL')\n",
    "    print(f\"base_url={base_url}\")\n",
    "    if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        return\n",
    "\n",
    "    llama_client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "    # Define models to test\n",
    "    # make sure they are available in your LLS server\n",
    "    models = [\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "\n",
    "    client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries_bad_functions.json\")\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        if client_tool_queries:\n",
    "            queries = load_queries(client_tool_queries)\n",
    "\n",
    "            if not queries:\n",
    "                logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "                continue\n",
    "\n",
    "            for query_obj in queries:\n",
    "                total_tests += 1\n",
    "                success = run_client_tool_test(model, query_obj, tools_bad_function_names, llama_client, logger)\n",
    "                if success:\n",
    "                    successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots('./results/bad_function_names_client_tool_metrics.csv')\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "id": "afd3207af6490e75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n",
    "![bad_function_names_tool_call_match_per_function_tool](results/plots/bad_function_names_tool_call_match_per_function_tool.jpg)\n",
    "\n",
    "The results show a sharp degrade in accuracy, emphasizing the importance of good function naming practices. Another experiment which could spawn from this is seeing whether using unit test style function naming for client tools and MCP servers."
   ],
   "id": "20eb271445aee556"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This next cell will test whether the tool description matters at all. To do this test, all docstrings have been reduced to only contain the required `:params:` annotation and function names have been kept the same.\n",
    "\n",
    "Ex.\n",
    "```python\n",
    "@client_tool\n",
    "def add_two_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```"
   ],
   "id": "7e73ab1372d7ba89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import utils\n",
    "from llama_stack_client.lib.agents.client_tool import ClientTool\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from tools import tools_only_params\n",
    "\n",
    "# Max client tools llama3.2:3B can handle\n",
    "TOOL_LIMIT = 32\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def load_queries(file_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Load query strings from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        if not isinstance(data, dict) or 'queries' not in data:\n",
    "            raise ValueError(f\"Invalid JSON format in {file_path}\")\n",
    "\n",
    "        # Return full query objects with ID for better test identification\n",
    "        return data['queries']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Query file not found: {file_path}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def get_query_id(query_obj):\n",
    "    \"\"\"Extract an ID from a query object for better test identification.\"\"\"\n",
    "    if isinstance(query_obj, dict) and 'id' in query_obj:\n",
    "        return query_obj['id']\n",
    "    elif isinstance(query_obj, dict) and 'query' in query_obj:\n",
    "        # Use first few words of query if no ID is available\n",
    "        words = query_obj['query'].split()[:5]\n",
    "        return '_'.join(words).lower().replace(',', '').replace('.', '')\n",
    "    return \"unknown_query\"\n",
    "\n",
    "def execute_query(\n",
    "    client: LlamaStackClient,\n",
    "    prompt: str,\n",
    "    model: str,\n",
    "    tools: Union[List[str], List[Any]], # list of toolgroup_ids or tool objects\n",
    "    instructions: Optional[str] = None,\n",
    "    max_tokens: int = 4096\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute a single query with a given set of tools.\"\"\"\n",
    "\n",
    "    if instructions is None:\n",
    "        # Default instructions for general tool use\n",
    "        instructions = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "            Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\n",
    "            When you are asked to search the web you must use a tool. Keep answers concise.\n",
    "            \"\"\"\n",
    "\n",
    "    # Create agent with tools\n",
    "    agent = Agent(\n",
    "        client,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=tools,\n",
    "        tool_config={\"tool_choice\": \"auto\"},\n",
    "        sampling_params={\"max_tokens\": max_tokens}\n",
    "    )\n",
    "\n",
    "    # Create session\n",
    "    session_id = agent.create_session(session_name=f\"Test_{int(time.time())}\")\n",
    "    print(f\"Created session_id={session_id} for Agent({agent.agent_id})\" if not all(isinstance(t, str) for t in tools) else \"\")\n",
    "\n",
    "    turn_response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    return turn_response\n",
    "\n",
    "def run_client_tool_test(model, query_obj, client_tool_module, llama_client, logger):\n",
    "    \"\"\"Run a single test for a specific model and query.\"\"\"\n",
    "    query_id = get_query_id(query_obj)\n",
    "    prompt = query_obj['query']\n",
    "    expected_tool_call = query_obj['tool_call']\n",
    "\n",
    "    tool_list = []\n",
    "    for name in dir(client_tool_module):\n",
    "        if len(tool_list) >= TOOL_LIMIT:\n",
    "            break\n",
    "        attribute = getattr(client_tool_module, name)\n",
    "        if isinstance(attribute, ClientTool):\n",
    "            tool_list.append(attribute)\n",
    "\n",
    "    logger.info(f\"Testing query '{query_id}' with model {model}\")\n",
    "    logger.info(f\"Query: {prompt[:50]}...\")\n",
    "\n",
    "    try:\n",
    "        response = execute_query(\n",
    "            client=llama_client,\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            tools=tool_list,\n",
    "        )\n",
    "        # Get Tool execution and Inference steps\n",
    "        steps = response.steps\n",
    "\n",
    "        #Get tool used\n",
    "        try:\n",
    "            tools_used = steps[1].tool_calls[0].tool_name\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting tool name: {e}\")\n",
    "            tools_used = None\n",
    "        tool_call_match = True if tools_used == expected_tool_call else False\n",
    "        logger.info(f\"Tool used: {tools_used} Tool expected: {expected_tool_call} match: {tool_call_match} \")\n",
    "\n",
    "        #Check inference was not empty\n",
    "        final_response = \"\"\n",
    "        try:\n",
    "            final_response = steps[2].api_model_response.content.strip()\n",
    "            inference_not_empty = True if final_response != '' else False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking inference content: {e}\")\n",
    "            inference_not_empty = False\n",
    "        logger.info(f'Inference not empty: {inference_not_empty}')\n",
    "        logger.info(f\"Query '{query_id}' succeeded with model {model} and the response \\n {final_response}\")\n",
    "\n",
    "        # Record success metrics, including the expected_tool_call\n",
    "        utils.add_metric(\n",
    "            model=model,\n",
    "            query_id=query_id,\n",
    "            status=\"SUCCESS\",\n",
    "            tool_call_match=tool_call_match,\n",
    "            inference_not_empty=inference_not_empty,\n",
    "            expected_tool_call=expected_tool_call\n",
    "        )\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        logger.error(f\"Query '{query_id}' failed with model {model}: {error_msg}\")\n",
    "\n",
    "        # Record failure metrics\n",
    "        utils.add_metric(\n",
    "            model=model,\n",
    "            query_id=query_id,\n",
    "            status=\"FAILURE\",\n",
    "            tool_call_match=False,\n",
    "            inference_not_empty=False,\n",
    "            expected_tool_call=expected_tool_call,\n",
    "            error=error_msg\n",
    "        )\n",
    "\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run all tests.\"\"\"\n",
    "    # Set up logger\n",
    "    logger = utils.setup_logger()\n",
    "\n",
    "    # Create client\n",
    "    base_url = os.getenv('REMOTE_BASE_URL')\n",
    "    print(f\"base_url={base_url}\")\n",
    "    if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        return\n",
    "\n",
    "    llama_client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "    # Define models to test\n",
    "    # make sure they are available in your LLS server\n",
    "    models = [\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "\n",
    "    client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries.json\")\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        if client_tool_queries:\n",
    "            queries = load_queries(client_tool_queries)\n",
    "\n",
    "            if not queries:\n",
    "                logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "                continue\n",
    "\n",
    "            for query_obj in queries:\n",
    "                total_tests += 1\n",
    "                success = run_client_tool_test(model, query_obj, tools_only_params, llama_client, logger)\n",
    "                if success:\n",
    "                    successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots('./results/only_params_client_tool_metrics.csv')\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "id": "84ce35871570aa05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![only_params_tool_call_match_per_function.jpg](results/plots/only_params_tool_call_match_per_function.jpg)\n",
    "\n",
    "The results show that removing all details from the docstring other than the required `:params:` annotation does not lead to large decrease in accuracy. This is likely why `llama-stack` only requires the `:params:` annotation but nothing else, like `:use_case:`."
   ],
   "id": "bee5ffa29c5b3e20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## We will now run the same evaluation set on the well constructed tools but swap `llama3.2:3B` with `llama3.2:1B`.",
   "id": "ec74daaf4026b94f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import utils\n",
    "from llama_stack_client.lib.agents.client_tool import ClientTool\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from tools import tools\n",
    "\n",
    "# Max client tools llama3.2:3B can handle\n",
    "TOOL_LIMIT = 23\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def load_queries(file_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Load query strings from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        if not isinstance(data, dict) or 'queries' not in data:\n",
    "            raise ValueError(f\"Invalid JSON format in {file_path}\")\n",
    "\n",
    "        # Return full query objects with ID for better test identification\n",
    "        return data['queries']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Query file not found: {file_path}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def get_query_id(query_obj):\n",
    "    \"\"\"Extract an ID from a query object for better test identification.\"\"\"\n",
    "    if isinstance(query_obj, dict) and 'id' in query_obj:\n",
    "        return query_obj['id']\n",
    "    elif isinstance(query_obj, dict) and 'query' in query_obj:\n",
    "        # Use first few words of query if no ID is available\n",
    "        words = query_obj['query'].split()[:5]\n",
    "        return '_'.join(words).lower().replace(',', '').replace('.', '')\n",
    "    return \"unknown_query\"\n",
    "\n",
    "def execute_query(\n",
    "    client: LlamaStackClient,\n",
    "    prompt: str,\n",
    "    model: str,\n",
    "    tools: Union[List[str], List[Any]], # list of toolgroup_ids or tool objects\n",
    "    instructions: Optional[str] = None,\n",
    "    max_tokens: int = 4096\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute a single query with a given set of tools.\"\"\"\n",
    "\n",
    "    if instructions is None:\n",
    "        # Default instructions for general tool use\n",
    "        instructions = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "            Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\n",
    "            When you are asked to search the web you must use a tool. Keep answers concise.\n",
    "            \"\"\"\n",
    "\n",
    "    # Create agent with tools\n",
    "    agent = Agent(\n",
    "        client,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=tools,\n",
    "        tool_config={\"tool_choice\": \"auto\"},\n",
    "        sampling_params={\"max_tokens\": max_tokens}\n",
    "    )\n",
    "\n",
    "    # Create session\n",
    "    session_id = agent.create_session(session_name=f\"Test_{int(time.time())}\")\n",
    "    print(f\"Created session_id={session_id} for Agent({agent.agent_id})\" if not all(isinstance(t, str) for t in tools) else \"\")\n",
    "\n",
    "    turn_response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=False\n",
    "    )\n",
    "    return turn_response\n",
    "\n",
    "def run_client_tool_test(model, query_obj, client_tool_module, llama_client, logger):\n",
    "    \"\"\"Run a single test for a specific model and query.\"\"\"\n",
    "    query_id = get_query_id(query_obj)\n",
    "    prompt = query_obj['query']\n",
    "    expected_tool_call = query_obj['tool_call']\n",
    "\n",
    "    tool_list = []\n",
    "    for name in dir(client_tool_module):\n",
    "        if len(tool_list) >= TOOL_LIMIT:\n",
    "            break\n",
    "        attribute = getattr(client_tool_module, name)\n",
    "        if isinstance(attribute, ClientTool):\n",
    "            tool_list.append(attribute)\n",
    "\n",
    "    logger.info(f\"Testing query '{query_id}' with model {model}\")\n",
    "    logger.info(f\"Query: {prompt[:50]}...\")\n",
    "\n",
    "    try:\n",
    "        response = execute_query(\n",
    "            client=llama_client,\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            tools=tool_list,\n",
    "        )\n",
    "        # Get Tool execution and Inference steps\n",
    "        steps = response.steps\n",
    "\n",
    "        #Get tool used\n",
    "        try:\n",
    "            tools_used = steps[1].tool_calls[0].tool_name\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting tool name: {e}\")\n",
    "            tools_used = None\n",
    "        tool_call_match = True if tools_used == expected_tool_call else False\n",
    "        logger.info(f\"Tool used: {tools_used} Tool expected: {expected_tool_call} match: {tool_call_match} \")\n",
    "\n",
    "        #Check inference was not empty\n",
    "        final_response = \"\"\n",
    "        try:\n",
    "            final_response = steps[2].api_model_response.content.strip()\n",
    "            inference_not_empty = True if final_response != '' else False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking inference content: {e}\")\n",
    "            inference_not_empty = False\n",
    "        logger.info(f'Inference not empty: {inference_not_empty}')\n",
    "        logger.info(f\"Query '{query_id}' succeeded with model {model} and the response \\n {final_response}\")\n",
    "\n",
    "        # Record success metrics, including the expected_tool_call\n",
    "        utils.add_metric(\n",
    "            model=model,\n",
    "            query_id=query_id,\n",
    "            status=\"SUCCESS\",\n",
    "            tool_call_match=tool_call_match,\n",
    "            inference_not_empty=inference_not_empty,\n",
    "            expected_tool_call=expected_tool_call\n",
    "        )\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        logger.error(f\"Query '{query_id}' failed with model {model}: {error_msg}\")\n",
    "\n",
    "        # Record failure metrics\n",
    "        utils.add_metric(\n",
    "            model=model,\n",
    "            query_id=query_id,\n",
    "            status=\"FAILURE\",\n",
    "            tool_call_match=False,\n",
    "            inference_not_empty=False,\n",
    "            expected_tool_call=expected_tool_call,\n",
    "            error=error_msg\n",
    "        )\n",
    "\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run all tests.\"\"\"\n",
    "    # Set up logger\n",
    "    logger = utils.setup_logger()\n",
    "\n",
    "    # Create client\n",
    "    base_url = os.getenv('REMOTE_BASE_URL')\n",
    "    print(f\"base_url={base_url}\")\n",
    "    if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        return\n",
    "\n",
    "    llama_client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "    # Define models to test\n",
    "    # make sure they are available in your LLS server\n",
    "    models = [\"llama3.2:1b\"]\n",
    "\n",
    "    client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries.json\")\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        if client_tool_queries:\n",
    "            queries = load_queries(client_tool_queries)\n",
    "\n",
    "            if not queries:\n",
    "                logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "                continue\n",
    "\n",
    "            for query_obj in queries:\n",
    "                total_tests += 1\n",
    "                success = run_client_tool_test(model, query_obj, tools, llama_client, logger)\n",
    "                if success:\n",
    "                    successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots('./results/normal_client_tool_metrics_1B.csv')\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "id": "ca60c1aea5b97699"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n",
    "![tool_calling_match_per_function_23_tools_1B.png.jpg](results/plots/tool_calling_match_per_function_23_tools_1B.png.jpg)\n",
    "![tool_calling_match_per_function_1_tool_1B.jpg](results/plots/tool_calling_match_per_function_1_tool_1B.jpg)\n",
    "\n",
    "The results show that 1B param models, at least for llama family, are too small for tool calling.\n",
    "\n",
    "Another test was done where the model was **explicitly** told to use tool calling and given **only** the correct tool, in order to test whether the tool selection process was offloaded if 1B param models would work, however surprisingly it got 0 successful tool calls then."
   ],
   "id": "ba8bab8a04bd6532"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "The following observations have been made using our eval set of 600 queries.\n",
    "- `llama3.2:3B` can handle a maximum of 32 tools before a complete degradation in accuracy.\n",
    "- Well named functions are exponentially more important than a well written function description.\n",
    "- Explicitly added `:description:` and `:use_case:` showed a slight improve in accuracy for our eval set.\n",
    "- LLMs need to be introduced to tool calling in the training data, otherwise will have difficulty correctly calling them.\n",
    "    - This was seen by pulling `granite3.2` from ollama, and it wasn't able to call a single tool.\n",
    "- `llama3.2:1B` is too small of a model and is extremely inconsistent at tool calling.\n",
    "    - Even given just a single tool and explicitly instructed to call that tool, it is unable to consistently do it. Important note is that the quantized model was used which could have been a big factor for low performance"
   ],
   "id": "b07170f3bdc48090"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
