{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluating Tool Calling Limitations and Performance of Small LLMs\n",
    "Goal: The primary goal of this experiment is to evaluate the tool calling limitations of small LLMs (1B - 3B parameters) and to identify methods (e.g., prompting, tool descriptions) to enhance their performance.\n",
    "\n",
    "Evaluation Set: This analysis uses a custom evaluation set comprising 600 queries. The queries were all generated by Gemini, there are 15 queries per function tool and a total of 40 function tools."
   ],
   "id": "f02eceeb0d3f3235"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This initial experiment demonstrates that `llama3.2:3B` exhibits a complete degradation in accuracy when provided with more than 32 tools. The `TOOL_LIMIT` is set to 32 because any increase beyond this number results in a complete loss of tool-calling accuracy, which was a surprising outcome given that one might intuitively expect only a decrease in performance.",
   "id": "21863d62bc6413e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import utils\n",
    "from dotenv import load_dotenv\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from tools import tools, tools_only_params, tools_no_extra_tags, tools_bad_function_names\n",
    "from tests import load_queries, run_client_tool_test\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "81dd26b44d0990c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set tool limit to max for llama3.2:3B\n",
    "TOOL_LIMIT = 32"
   ],
   "id": "3d96bc651a4e8aeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run all tests.\"\"\"\n",
    "    # Set up logger\n",
    "    logger = utils.setup_logger()\n",
    "\n",
    "    # Create client\n",
    "    base_url = os.getenv('REMOTE_BASE_URL')\n",
    "    print(f\"base_url={base_url}\")\n",
    "    if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        return\n",
    "\n",
    "    llama_client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "    # Define models to test\n",
    "    # make sure they are available in your LLS server\n",
    "    models = [\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "\n",
    "    client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries.json\")\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        if client_tool_queries:\n",
    "            queries = load_queries(client_tool_queries)\n",
    "\n",
    "            if not queries:\n",
    "                logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "                continue\n",
    "\n",
    "            for query_obj in queries:\n",
    "                total_tests += 1\n",
    "                success = run_client_tool_test(model, query_obj, tools_no_extra_tags, llama_client, logger)\n",
    "                if success:\n",
    "                    successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots('./results/no_extra_tools_client_tool_metrics.csv')\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "id": "32cc19b7d46ad666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![no_extra_tags_tool_call_match_per_function_tool.jpg](results/plots/no_extra_tags_tool_call_match_per_function_tool.jpg)\n",
    "\n",
    "Based off the results, the llama3.2:3B model has quite high accuracy by just giving a good function name and `:params:` in the docstring"
   ],
   "id": "4f7fe564688ff04f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The next cell will show how adding a single tool after 32 will lead to complete accuracy loss",
   "id": "45687c818fed8fb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set tool limit to one more than the max (32)\n",
    "TOOL_LIMIT = 33"
   ],
   "id": "a3b82f4ea17308dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main()",
   "id": "dbd41e175d20bd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![tool_calling_match_per_function_33_tools_3B](results/plots/tool_calling_match_per_function_33_tools_3B.jpg)\n",
    "This shows how adding a single tool after 32 completely degrades the accuracy of successful tool calls to almost 0."
   ],
   "id": "2d097a7b9b888178"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This next cell will test how adding explicit `:description:` and `:use_case:` annotations can help in increasing accuracy. It is **not** a fact that adding them will increase accuracy but for our query set it helped.",
   "id": "ccc06f630075723e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set tool limit back to max for llama3.2:3B\n",
    "TOOL_LIMIT = 32"
   ],
   "id": "112ae1854ff85184",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run all tests.\"\"\"\n",
    "    # Set up logger\n",
    "    logger = utils.setup_logger()\n",
    "\n",
    "    # Create client\n",
    "    base_url = os.getenv('REMOTE_BASE_URL')\n",
    "    print(f\"base_url={base_url}\")\n",
    "    if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        return\n",
    "\n",
    "    llama_client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "    # Define models to test\n",
    "    # make sure they are available in your LLS server\n",
    "    models = [\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "\n",
    "    client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries.json\")\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        if client_tool_queries:\n",
    "            queries = load_queries(client_tool_queries)\n",
    "\n",
    "            if not queries:\n",
    "                logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "                continue\n",
    "\n",
    "            for query_obj in queries:\n",
    "                total_tests += 1\n",
    "                success = run_client_tool_test(model, query_obj, tools, llama_client, logger)\n",
    "                if success:\n",
    "                    successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots('./results/normal_client_tool_metrics.csv')\n",
    "\n",
    "\n",
    "main()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![Tool Call Match Per Function Tool](results/plots/normal_tool_call_match_per_function_tool.png)\n",
    "Overall majority of the tools have 100% accuracy, the tool with the worst accuracy is `convert_fahrenheit_to_kelvin`. One possible explanation could be that the training data likely had very little, if any, data on this type of Q/A. `convert_celsius_to_kelvin` has 100% accuracy which supports this theory as that's a common conversion in science courses."
   ],
   "id": "65757d655859540f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The next few cells run experiments to test what truly matters when defining a tool: the tool name, description, and the format of the docstring. A quick overview of how `llama-stack` parses the function when tagged with the `client_tool` decorator.\n",
    "\n",
    "```python\n",
    "def client_tool(func: T) -> ClientTool:\n",
    "    \"\"\"\n",
    "    Decorator to convert a function into a ClientTool.\n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "    class _WrappedTool(ClientTool):\n",
    "        __name__ = func.__name__\n",
    "        __doc__ = func.__doc__\n",
    "        __module__ = func.__module__\n",
    "\n",
    "        def get_name(self) -> str:\n",
    "            ...\n",
    "\n",
    "        def get_description(self) -> str:\n",
    "            ...\n",
    "\n",
    "        def get_params_definition(self) -> Dict[str, Parameter]:\n",
    "            hints = get_type_hints(func)\n",
    "            # Remove return annotation if present\n",
    "            hints.pop(\"return\", None)\n",
    "\n",
    "            # Get parameter descriptions from docstring\n",
    "            params = {}\n",
    "            sig = inspect.signature(func)\n",
    "            doc = inspect.getdoc(func) or \"\"\n",
    "\n",
    "            for name, type_hint in hints.items():\n",
    "                # Look for :param name: in docstring\n",
    "                param_doc = \"\"\n",
    "                for line in doc.split(\"\\n\"):\n",
    "                    if line.strip().startswith(f\":param {name}:\"):\n",
    "                        param_doc = line.split(\":\", 2)[2].strip()\n",
    "                        break\n",
    "\n",
    "                if param_doc == \"\":\n",
    "                    raise ValueError(f\"No parameter description found for parameter {name}\")\n",
    "\n",
    "                ...\n",
    "\n",
    "            return params\n",
    "```\n",
    "Full implementation can be found [here](https://github.com/meta-llama/llama-stack-client-python/blob/645d2195c5af1c6f903cb93c293319d8f94c36cc/src/llama_stack_client/lib/agents/client_tool.py#L150-L170).\n",
    "An important thing to realize is that `llama-stack` **purposefully** disregards the return information from the docstring. Also that the docstring only **requires** one annotation, `:params:`, and everything _above_ that will be parsed as well."
   ],
   "id": "765e19f8bba53e6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This next cell will test whether explicitly having `:description:` and `:use_case:` annotations help, compared to including them without any annotation.\n",
    "\n",
    "Ex.\n",
    "```python\n",
    "@client_tool\n",
    "def add_two_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    :description: Adds two numbers.\n",
    "    :use_case: Use when the user wants to find the sum, total, or combined value of two numbers.\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    :returns: The sum of `a` and `b`.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```\n",
    "\n",
    "compared to\n",
    "\n",
    "```python\n",
    "@client_tool\n",
    "def add_two_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Adds two numbers.\n",
    "    Use when the user wants to find the sum, total, or combined value of two numbers.\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    :returns: The sum of `a` and `b`.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```"
   ],
   "id": "466e2edfe4fdf8fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This next cell will test whether the tool name matters at all. To do this test, all functions were renamed to `function_1`, `function_2`, etc. but the docstring was left unchanged.\n",
    "\n",
    "Ex.\n",
    "```python\n",
    "@client_tool\n",
    "def function_1(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    :description: Adds two numbers.\n",
    "    :use_case: Use when the user wants to find the sum, total, or combined value of two numbers.\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    :returns: The sum of `a` and `b`.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```"
   ],
   "id": "ee5f00dc9a51ee1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run all tests.\"\"\"\n",
    "    # Set up logger\n",
    "    logger = utils.setup_logger()\n",
    "\n",
    "    # Create client\n",
    "    base_url = os.getenv('REMOTE_BASE_URL')\n",
    "    print(f\"base_url={base_url}\")\n",
    "    if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        return\n",
    "\n",
    "    llama_client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "    # Define models to test\n",
    "    # make sure they are available in your LLS server\n",
    "    models = [\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "\n",
    "    client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries_bad_functions.json\")\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        if client_tool_queries:\n",
    "            queries = load_queries(client_tool_queries)\n",
    "\n",
    "            if not queries:\n",
    "                logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "                continue\n",
    "\n",
    "            for query_obj in queries:\n",
    "                total_tests += 1\n",
    "                success = run_client_tool_test(model, query_obj, tools_bad_function_names, llama_client, logger)\n",
    "                if success:\n",
    "                    successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots('./results/bad_function_names_client_tool_metrics.csv')\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "id": "afd3207af6490e75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n",
    "![bad_function_names_tool_call_match_per_function_tool](results/plots/bad_function_names_tool_call_match_per_function_tool.jpg)\n",
    "\n",
    "The results show a sharp degrade in accuracy, emphasizing the importance of good function naming practices. Another experiment which could spawn from this is seeing whether using unit test style function naming for client tools and MCP servers."
   ],
   "id": "20eb271445aee556"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This next cell will test whether the tool description matters at all. To do this test, all docstrings have been reduced to only contain the required `:params:` annotation and function names have been kept the same.\n",
    "\n",
    "Ex.\n",
    "```python\n",
    "@client_tool\n",
    "def add_two_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    :param a: The first number.\n",
    "    :param b: The second number.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "```"
   ],
   "id": "7e73ab1372d7ba89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run all tests.\"\"\"\n",
    "    # Set up logger\n",
    "    logger = utils.setup_logger()\n",
    "\n",
    "    # Create client\n",
    "    base_url = os.getenv('REMOTE_BASE_URL')\n",
    "    print(f\"base_url={base_url}\")\n",
    "    if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        return\n",
    "\n",
    "    llama_client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "    # Define models to test\n",
    "    # make sure they are available in your LLS server\n",
    "    models = [\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "\n",
    "    client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries.json\")\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        if client_tool_queries:\n",
    "            queries = load_queries(client_tool_queries)\n",
    "\n",
    "            if not queries:\n",
    "                logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "                continue\n",
    "\n",
    "            for query_obj in queries:\n",
    "                total_tests += 1\n",
    "                success = run_client_tool_test(model, query_obj, tools_only_params, llama_client, logger)\n",
    "                if success:\n",
    "                    successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots('./results/only_params_client_tool_metrics.csv')\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "id": "84ce35871570aa05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "![only_params_tool_call_match_per_function.jpg](results/plots/only_params_tool_call_match_per_function.jpg)\n",
    "\n",
    "The results show that removing all details from the docstring other than the required `:params:` annotation does not lead to large decrease in accuracy. This is likely why `llama-stack` only requires the `:params:` annotation but nothing else, like `:use_case:`."
   ],
   "id": "bee5ffa29c5b3e20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## We will now run the same evaluation set on the well constructed tools but swap `llama3.2:3B` with `llama3.2:1B`.",
   "id": "ec74daaf4026b94f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run all tests.\"\"\"\n",
    "    # Set up logger\n",
    "    logger = utils.setup_logger()\n",
    "\n",
    "    # Create client\n",
    "    base_url = os.getenv('REMOTE_BASE_URL')\n",
    "    print(f\"base_url={base_url}\")\n",
    "    if not base_url:\n",
    "        logger.error(\"REMOTE_BASE_URL environment variable not set\")\n",
    "        return\n",
    "\n",
    "    llama_client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "    # Define models to test\n",
    "    # make sure they are available in your LLS server\n",
    "    models = [\"llama3.2:1b\"]\n",
    "\n",
    "    client_tool_queries = os.path.join(os.getcwd(), \"queries/\", \"client_tool_queries.json\")\n",
    "\n",
    "    # Track statistics\n",
    "    total_tests = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    # Loop through models (outermost loop)\n",
    "    for model in models:\n",
    "        logger.info(f\"\\n=== Testing with model: {model} ===\\n\")\n",
    "\n",
    "        if client_tool_queries:\n",
    "            queries = load_queries(client_tool_queries)\n",
    "\n",
    "            if not queries:\n",
    "                logger.info(f\"No queries found in {client_tool_queries}\")\n",
    "                continue\n",
    "\n",
    "            for query_obj in queries:\n",
    "                total_tests += 1\n",
    "                success = run_client_tool_test(model, query_obj, tools, llama_client, logger)\n",
    "                if success:\n",
    "                    successful_tests += 1\n",
    "\n",
    "    # Print summary\n",
    "    logger.info(f\"\\n=== Test Summary ===\")\n",
    "    logger.info(f\"Total tests: {total_tests}\")\n",
    "    logger.info(f\"Successful tests: {successful_tests}\")\n",
    "    logger.info(f\"Failed tests: {total_tests - successful_tests}\")\n",
    "    if total_tests > 0:\n",
    "        success_rate = (successful_tests / total_tests) * 100\n",
    "        logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    # Generate plots\n",
    "    logger.info(f\"\\n=== Generating plots ===\")\n",
    "    utils.get_analysis_plots('./results/normal_client_tool_metrics_1B.csv')\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "id": "ca60c1aea5b97699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n",
    "![tool_calling_match_per_function_23_tools_1B.png.jpg](results/plots/tool_calling_match_per_function_23_tools_1B.png.jpg)\n",
    "\n",
    "The results show that llama3.2:1B is far worse at tool calling compared to llama3.2:3B"
   ],
   "id": "ba8bab8a04bd6532"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "The following observations have been made using our eval set of 600 queries.\n",
    "- `llama3.2:3B` can handle a maximum of 32 tools before a complete degradation in accuracy.\n",
    "- Well named functions are more important than a well written function description.\n",
    "- Explicitly added `:description:` and `:use_case:` showed a slight improve in accuracy for our eval set.\n",
    "- LLMs need to be introduced to tool calling in the training data, otherwise will have difficulty correctly calling them.\n",
    "    - This was seen by pulling `granite3.2` from ollama, and it wasn't able to call a single tool.\n",
    "- `llama3.2:1B` is too small of a model and is extremely inconsistent at tool calling.\n",
    "    - Even given just a single tool and explicitly instructed to call that tool, it is unable to consistently do it. Important note is that the quantized model was used which could have been a big factor for low performance"
   ],
   "id": "b07170f3bdc48090"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
